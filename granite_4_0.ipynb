{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# mypy: allow-untyped-defs\n",
        "\"\"\"\n",
        "This module implements Paged Attention on top of flex_attention.\n",
        "This module is experimental and subject to change.\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "from torch.nn.attention.flex_attention import (\n",
        "    _identity,\n",
        "    _mask_mod_signature,\n",
        "    _score_mod_signature,\n",
        "    BlockMask,\n",
        "    noop_mask,\n",
        "    flex_attention,\n",
        "    create_block_mask,\n",
        ")\n",
        "\n",
        "__all__ = [\"PagedAttention\"]\n",
        "\n",
        "\n",
        "def _cdiv(x: int | float | torch.Tensor, multiple: int | float | torch.Tensor):\n",
        "    return (x + multiple - 1) // multiple\n",
        "\n",
        "\n",
        "class PagedAttention:\n",
        "    \"\"\"\n",
        "    PagedAttention supports flex attention inference with a large batch size.\n",
        "    With PagedAttention, a batch of key/value tensors with varying kv length\n",
        "    is split into tensor blocks of fixed length and cached in a compact way.\n",
        "    Thus we can avoid redundant memory consumption due to varying kv length and\n",
        "    support a larger batch size.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_pages: int,\n",
        "        page_size: int,\n",
        "        max_batch_size: int,\n",
        "        device: str = \"cuda\",\n",
        "    ) -> None:\n",
        "        # number of pages\n",
        "        self.n_pages = n_pages\n",
        "\n",
        "        # number of tokens per page\n",
        "        self.page_size = page_size\n",
        "\n",
        "        # page table: [batch, logical_block_idx] -> physical_page_idx\n",
        "        self.page_table = -torch.ones(\n",
        "            (max_batch_size, self.n_pages), dtype=torch.int64, device=device\n",
        "        )\n",
        "\n",
        "        # capacity: batch_idx -> allocated sequence length\n",
        "        self.capacity = torch.zeros(max_batch_size, dtype=torch.int64, device=device)\n",
        "\n",
        "        # index of empty pages that is available for allocation\n",
        "        self.empty_pages = list(range(n_pages - 1, -1, -1))\n",
        "\n",
        "        # mapping from physical page index to logical page index\n",
        "        self.physical_to_logical = -torch.ones(\n",
        "            (max_batch_size, n_pages), dtype=torch.int64, device=device\n",
        "        )\n",
        "\n",
        "    def reserve(self, batch_idx: torch.Tensor, seq_len: torch.Tensor) -> None:\n",
        "        \"\"\"\n",
        "        Requests the capacity of a given batch to be at least enough to\n",
        "        hold `seq_len` elements.\n",
        "\n",
        "        Args:\n",
        "            batch_idx (Tensor): batch index to be reserved; shape :math:`(1)`.\n",
        "            seq_len (Tensor): minimum capacity for the given batch; shape :math:`(1)`.\n",
        "        \"\"\"\n",
        "\n",
        "        if seq_len <= self.capacity[batch_idx]:\n",
        "            return\n",
        "\n",
        "        num_pages_to_allocate = _cdiv(\n",
        "            seq_len - self.capacity[batch_idx], self.page_size\n",
        "        )\n",
        "\n",
        "        if len(self.empty_pages) < num_pages_to_allocate:\n",
        "            raise AssertionError(\n",
        "                f\"requested {num_pages_to_allocate.item()} pages \"\n",
        "                f\"but there are only {len(self.empty_pages)} empty pages\"\n",
        "            )\n",
        "\n",
        "        start_page_idx = self.capacity[batch_idx] // self.page_size\n",
        "        end_page_idx = start_page_idx + num_pages_to_allocate\n",
        "\n",
        "        # find empty physical pages\n",
        "        allocated_pages = torch.tensor(\n",
        "            self.empty_pages[-num_pages_to_allocate:],\n",
        "            device=num_pages_to_allocate.device,\n",
        "        )\n",
        "        self.empty_pages = self.empty_pages[:-num_pages_to_allocate]\n",
        "\n",
        "        # update page table\n",
        "        self.page_table[\n",
        "            batch_idx,\n",
        "            start_page_idx:end_page_idx,\n",
        "        ] = allocated_pages\n",
        "\n",
        "        # update metadata\n",
        "        self.physical_to_logical[batch_idx, allocated_pages] = torch.arange(\n",
        "            start_page_idx.item(),\n",
        "            end_page_idx.item(),\n",
        "            device=num_pages_to_allocate.device,\n",
        "        )\n",
        "        self.capacity[batch_idx] += num_pages_to_allocate * self.page_size\n",
        "\n",
        "    def erase(self, batch_idx: torch.Tensor) -> None:\n",
        "        \"\"\"\n",
        "        Removes a single batch from paged attention.\n",
        "\n",
        "        Args:\n",
        "            batch_idx (Tensor): batch index to be removed; shape :math:`(1)`.\n",
        "        \"\"\"\n",
        "\n",
        "        # find allocated pages\n",
        "        allocated_page_idx = self.page_table[batch_idx] != -1\n",
        "        allocated_pages = self.page_table[batch_idx][allocated_page_idx]\n",
        "\n",
        "        # clean metadata\n",
        "        self.capacity[batch_idx] = 0\n",
        "        self.empty_pages += allocated_pages.tolist()\n",
        "        self.physical_to_logical[batch_idx][:, allocated_pages] = -1\n",
        "        self.page_table[batch_idx] = -1\n",
        "\n",
        "    def assign(\n",
        "        self,\n",
        "        batch_idx: torch.Tensor,\n",
        "        input_pos: torch.Tensor,\n",
        "        k_val: torch.Tensor,\n",
        "        v_val: torch.Tensor,\n",
        "        k_cache: torch.Tensor,\n",
        "        v_cache: torch.Tensor,\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Assigns new contents `val` to the storage `cache` at the location\n",
        "        `batch_idx` and `input_pos`.\n",
        "\n",
        "        Args:\n",
        "            batch_idx (Tensor): batch index; shape :math:`(B)`.\n",
        "            input_pos (Tensor): input positions to be assigned for the given batch; shape :math:`(B, S)`.\n",
        "            val (Tensor): value to be assigned; shape :math:`(B, H, S, D)`\n",
        "            cache (Tensor): the cache to store the values; shape:`(1, H, MAX_S, D)`\n",
        "        \"\"\"\n",
        "        if k_val.requires_grad:\n",
        "            raise RuntimeError(\"val must not require gradient\")\n",
        "\n",
        "        B, H, S, K_D = k_val.shape\n",
        "        V_D = v_val.shape[3]\n",
        "        if B != batch_idx.shape[0]:\n",
        "            raise RuntimeError(\n",
        "                f\"Expect val and batch_idx have the same batch size \"\n",
        "                f\"but got B={B} and B={batch_idx.shape[0]}.\"\n",
        "            )\n",
        "        if H != k_cache.shape[1]:\n",
        "            raise RuntimeError(\n",
        "                f\"Expect val and cache has the same number of heads \"\n",
        "                f\"but got H={H} and H={k_cache.shape[1]}.\"\n",
        "            )\n",
        "        if S != input_pos.shape[1]:\n",
        "            raise RuntimeError(\n",
        "                f\"Expect val and input_pos has the same length \"\n",
        "                f\"but got S={S} and S={input_pos.shape[0]}.\"\n",
        "            )\n",
        "        if K_D != k_cache.shape[3]:\n",
        "            raise RuntimeError(\n",
        "                f\"Expect k_val and k_cache has the same hidden dim \"\n",
        "                f\"but got D={K_D} and D={k_cache.shape[3]}.\"\n",
        "            )\n",
        "        if V_D != v_cache.shape[3]:\n",
        "            raise RuntimeError(\n",
        "                f\"Expect v_val and v_cache has the same hidden dim \"\n",
        "                f\"but got D={V_D} and D={v_cache.shape[3]}.\"\n",
        "            )\n",
        "\n",
        "        # find address\n",
        "        logical_block_idx = input_pos // self.page_size  # [B, S]\n",
        "        logical_block_offset = input_pos % self.page_size  # [B, S]\n",
        "        physical_block_idx = torch.gather(\n",
        "            self.page_table[batch_idx], 1, logical_block_idx.to(torch.int64)\n",
        "        ).to(torch.int32)  # [B, S]\n",
        "\n",
        "        addr = (physical_block_idx * self.page_size + logical_block_offset).view(\n",
        "            -1\n",
        "        )  # [B*S]\n",
        "\n",
        "        k_val = k_val.permute(1, 0, 2, 3).contiguous().view(1, H, B * S, K_D)\n",
        "        v_val = v_val.permute(1, 0, 2, 3).contiguous().view(1, H, B * S, V_D)\n",
        "\n",
        "        k_cache[:, :, addr, :] = k_val\n",
        "        v_cache[:, :, addr, :] = v_val\n",
        "\n",
        "    def convert_logical_block_mask(\n",
        "        self,\n",
        "        block_mask: BlockMask,\n",
        "        batch_idx: torch.Tensor | None = None,\n",
        "        kv_len: torch.Tensor | None = None,\n",
        "    ) -> BlockMask:\n",
        "        \"\"\"\n",
        "        Converts a logical block mask by mapping its logical kv indices to the corresponding\n",
        "        physical kv indices.\n",
        "\n",
        "        Args:\n",
        "            block_mask (BlockMask): logical block mask;\n",
        "                kv_indices shape :math:`(B, H, ROWS, MAX_BLOCKS_IN_COL)`.\n",
        "            batch_idx (Tensor): batch index corresponding to the block_mask\n",
        "                batch dimension. This provides flexibility to convert a\n",
        "                block mask with smaller batch size than the page table;\n",
        "                shape :math:`(B)`.\n",
        "            kv_len (Optional[Tensor]): actual KV sequence length for upper bound check;\n",
        "                shape :math:`(B,)` to handle multiple batches.\n",
        "        \"\"\"\n",
        "        B, H, ROWS, MAX_BLOCKS_IN_COL = block_mask.kv_indices.shape\n",
        "\n",
        "        if block_mask.BLOCK_SIZE[1] != self.page_size:\n",
        "            raise RuntimeError(\n",
        "                f\"Expect block_mask has the same column block size as page_size\"\n",
        "                f\"but got size={block_mask.BLOCK_SIZE[1]} and size={self.page_size}\"\n",
        "            )\n",
        "\n",
        "        device = block_mask.kv_num_blocks.device\n",
        "\n",
        "        if batch_idx is None:\n",
        "            batch_idx = torch.arange(B, device=device)\n",
        "        page_table = self.page_table[batch_idx]\n",
        "\n",
        "        new_kv_num_blocks = block_mask.kv_num_blocks.clone()\n",
        "\n",
        "        # The physical page table might be larger than the max logical blocks\n",
        "        # but the block mask only cares about MAX_BLOCKS_IN_COL.\n",
        "        # We assume the logical mask is sparse enough.\n",
        "        new_kv_indices = torch.zeros(\n",
        "            (B, H, ROWS, self.n_pages), dtype=torch.int32, device=device\n",
        "        )\n",
        "\n",
        "        # We gather the physical page indices using the logical block indices\n",
        "        # provided by the input block_mask.\n",
        "        new_kv_indices[:, :, :, :MAX_BLOCKS_IN_COL] = (\n",
        "            torch.gather(\n",
        "                page_table, 1, block_mask.kv_indices.view(B, -1).to(torch.int64)\n",
        "            )\n",
        "            .view(block_mask.kv_indices.shape)\n",
        "            .to(torch.int32)\n",
        "        )\n",
        "\n",
        "        new_full_kv_indices, new_full_kv_num_blocks = None, None\n",
        "        if block_mask.full_kv_num_blocks is not None:\n",
        "            if block_mask.full_kv_indices is None:\n",
        "                raise AssertionError(\n",
        "                    \"block_mask.full_kv_indices must not be None when full_kv_num_blocks is not None\"\n",
        "                )\n",
        "            new_full_kv_num_blocks = block_mask.full_kv_num_blocks.clone()\n",
        "            new_full_kv_indices = torch.zeros(\n",
        "                (B, H, ROWS, self.n_pages), dtype=torch.int32, device=device\n",
        "            )\n",
        "            new_full_kv_indices[:, :, :, :MAX_BLOCKS_IN_COL] = (\n",
        "                torch.gather(\n",
        "                    page_table,\n",
        "                    1,\n",
        "                    block_mask.full_kv_indices.view(B, -1).to(torch.int64),\n",
        "                )\n",
        "                .view(block_mask.full_kv_indices.shape)\n",
        "                .to(torch.int32)\n",
        "            )\n",
        "\n",
        "        new_mask_mod = self.get_mask_mod(block_mask.mask_mod, kv_len)\n",
        "\n",
        "        # The K/V cache sent to flex_attention acts as one giant sequence of length n_pages * page_size.\n",
        "        seq_lengths = (block_mask.seq_lengths[0], self.n_pages * self.page_size)\n",
        "\n",
        "        return BlockMask.from_kv_blocks(\n",
        "            new_kv_num_blocks,\n",
        "            new_kv_indices,\n",
        "            new_full_kv_num_blocks,\n",
        "            new_full_kv_indices,\n",
        "            block_mask.BLOCK_SIZE,\n",
        "            new_mask_mod,\n",
        "            seq_lengths=seq_lengths,\n",
        "        )\n",
        "\n",
        "    def get_mask_mod(\n",
        "        self,\n",
        "        mask_mod: _mask_mod_signature | None,\n",
        "        kv_len: torch.Tensor | None = None,\n",
        "    ) -> _mask_mod_signature:\n",
        "        \"\"\"\n",
        "        Converts a mask_mod based on mapping from the physical block index to the logical\n",
        "        block index.\n",
        "        \"\"\"\n",
        "        if mask_mod is None:\n",
        "            mask_mod = noop_mask\n",
        "\n",
        "        def new_mask_mod(\n",
        "            b: torch.Tensor,\n",
        "            h: torch.Tensor,\n",
        "            q_idx: torch.Tensor,\n",
        "            physical_kv_idx: torch.Tensor,\n",
        "        ):\n",
        "            physical_kv_block = physical_kv_idx // self.page_size\n",
        "            physical_kv_offset = physical_kv_idx % self.page_size\n",
        "            logical_block_idx = self.physical_to_logical[b, physical_kv_block]\n",
        "            logical_kv_idx = logical_block_idx * self.page_size + physical_kv_offset\n",
        "            live_block = logical_block_idx >= 0\n",
        "            within_upper_bound = (\n",
        "                logical_kv_idx < kv_len[b] if kv_len is not None else True\n",
        "            )\n",
        "            within_lower_bound = logical_kv_idx >= 0\n",
        "            is_valid = live_block & within_upper_bound & within_lower_bound\n",
        "\n",
        "            return torch.where(is_valid, mask_mod(b, h, q_idx, logical_kv_idx), False)\n",
        "\n",
        "        return new_mask_mod\n",
        "\n",
        "    def get_score_mod(\n",
        "        self,\n",
        "        score_mod: _score_mod_signature | None,\n",
        "        kv_len: torch.Tensor | None = None,\n",
        "    ) -> _score_mod_signature:\n",
        "        \"\"\"\n",
        "        Converts a score_mod based on mapping from the physical block index to the logical\n",
        "        block index.\n",
        "        \"\"\"\n",
        "        if score_mod is None:\n",
        "            score_mod = _identity\n",
        "\n",
        "        def new_score_mod(\n",
        "            score: torch.Tensor,\n",
        "            b: torch.Tensor,\n",
        "            h: torch.Tensor,\n",
        "            q_idx: torch.Tensor,\n",
        "            physical_kv_idx: torch.Tensor,\n",
        "        ):\n",
        "            physical_kv_block = physical_kv_idx // self.page_size\n",
        "            physical_kv_offset = physical_kv_idx % self.page_size\n",
        "            logical_block_idx = self.physical_to_logical[b, physical_kv_block]\n",
        "            logical_kv_idx = logical_block_idx * self.page_size + physical_kv_offset\n",
        "            live_block = logical_block_idx >= 0\n",
        "            within_upper_bound = (\n",
        "                logical_kv_idx < kv_len[b] if kv_len is not None else True\n",
        "            )\n",
        "            within_lower_bound = logical_kv_idx >= 0\n",
        "            is_valid = live_block & within_upper_bound & within_lower_bound\n",
        "\n",
        "            return torch.where(\n",
        "                is_valid,\n",
        "                score_mod(score, b, h, q_idx, logical_kv_idx),\n",
        "                float(\"-inf\"),\n",
        "            )\n",
        "\n",
        "        return new_score_mod\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        query: torch.Tensor,\n",
        "        k_cache: torch.Tensor,\n",
        "        v_cache: torch.Tensor,\n",
        "        kv_lens: torch.Tensor,\n",
        "        block_mask: BlockMask | None = None,\n",
        "        batch_indices: torch.Tensor | None = None,\n",
        "        score_mod: _score_mod_signature | None = None,\n",
        "        scale: float | None = None,\n",
        "        enable_gqa: bool = False,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Computes the paged attention.\n",
        "\n",
        "        Args:\n",
        "            query (Tensor): Query tensor; shape :math:`(B, H, Q_LEN, D)`.\n",
        "            k_cache (Tensor): Physical key cache; shape :math:`(1, H, TOTAL_PAGES * PAGE_SIZE, D)`.\n",
        "            v_cache (Tensor): Physical value cache; shape :math:`(1, H, TOTAL_PAGES * PAGE_SIZE, D)`.\n",
        "            kv_lens (Tensor): The logical length of the KV sequence for each batch element.\n",
        "                              Used to mask out padding and future tokens. Shape :math:`(B,)`.\n",
        "            block_mask (BlockMask, optional): Logical block mask. If None, a standard causal mask is assumed\n",
        "                                              for the logical sequence.\n",
        "            batch_indices (Tensor, optional): The indices in the page table corresponding to the query batch.\n",
        "                                              If None, assumes 0..B-1.\n",
        "            score_mod (callable, optional): Logical score modification function.\n",
        "            scale (float, optional): Attention scale factor.\n",
        "            enable_gqa (bool): Enable Grouped Query Attention.\n",
        "        \"\"\"\n",
        "        B, H, Q_LEN, D = query.shape\n",
        "        device = query.device\n",
        "\n",
        "        if batch_indices is None:\n",
        "            batch_indices = torch.arange(B, device=device)\n",
        "\n",
        "        # 1. Create a logical block mask if one isn't provided.\n",
        "        # We assume a standard Causal mask over the maximum logical KV length.\n",
        "        if block_mask is None:\n",
        "            max_kv_len = kv_lens.max().item()\n",
        "\n",
        "            def causal_mask(b, h, q, k):\n",
        "                return q >= k\n",
        "\n",
        "            # Create a block mask for the logical dimensions: (B, H, Q_LEN, Max_KV)\n",
        "            # Note: We must ensure the BLOCK_SIZE matches the page_size.\n",
        "            block_mask = create_block_mask(\n",
        "                causal_mask,\n",
        "                B=B,\n",
        "                H=H,\n",
        "                Q_LEN=Q_LEN,\n",
        "                KV_LEN=max_kv_len,\n",
        "                device=device,\n",
        "                BLOCK_SIZE=self.page_size,\n",
        "                _compile=True # Often improves performance for mask creation\n",
        "            )\n",
        "\n",
        "        # 2. Convert the logical block mask to a physical block mask.\n",
        "        # This re-maps the indices in the block mask to point to the correct physical pages.\n",
        "        physical_block_mask = self.convert_logical_block_mask(\n",
        "            block_mask,\n",
        "            batch_idx=batch_indices,\n",
        "            kv_len=kv_lens\n",
        "        )\n",
        "\n",
        "        # 3. Handle Score Mods\n",
        "        # Wrap the user score_mod (which expects logical indices) to handle physical indices.\n",
        "        physical_score_mod = self.get_score_mod(score_mod, kv_lens)\n",
        "\n",
        "        # 4. Call flex_attention\n",
        "        # Note: k_cache and v_cache have batch size 1, but query has batch size B.\n",
        "        # The block mask handles the routing (preventing batch B from seeing batch A's pages).\n",
        "        output = flex_attention(\n",
        "            query,\n",
        "            k_cache,\n",
        "            v_cache,\n",
        "            block_mask=physical_block_mask,\n",
        "            score_mod=physical_score_mod,\n",
        "            scale=scale,\n",
        "            enable_gqa=enable_gqa\n",
        "        )\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "BaSPnbP3hsjo"
      },
      "id": "BaSPnbP3hsjo",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.attention.flex_attention import create_block_mask\n",
        "\n",
        "# Assuming the PagedAttention class is defined in the same file or imported\n",
        "# from paged_attention import PagedAttention\n",
        "\n",
        "def test_paged_attention_correctness():\n",
        "    print(\"=== Starting Paged Attention Test ===\")\n",
        "\n",
        "    if not torch.cuda.is_available():\n",
        "        print(\"Skipping test: CUDA not available (FlexAttention requires CUDA).\")\n",
        "        return\n",
        "\n",
        "    device = \"cuda\"\n",
        "    torch.set_default_device(device)\n",
        "\n",
        "    # --- 1. Configuration ---\n",
        "    BATCH_SIZE = 4\n",
        "    NUM_HEADS = 6\n",
        "    HEAD_DIM = 64\n",
        "    PAGE_SIZE = 16\n",
        "    N_PAGES = 100  # Total physical pages available\n",
        "    Q_LEN = 12     # Query length (e.g., chunked prefill or decoding steps)\n",
        "\n",
        "    # Initialize Paged Attention Manager\n",
        "    pa = PagedAttention(\n",
        "        n_pages=N_PAGES,\n",
        "        page_size=PAGE_SIZE,\n",
        "        max_batch_size=BATCH_SIZE,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    # Allocate Physical Cache (Pre-allocated memory on GPU)\n",
        "    # Shape: [1, H, Total_Pages * Page_Size, D]\n",
        "    # Flex attention expects the cache to be treated as a single flattened sequence\n",
        "    physical_k_cache = torch.zeros(1, NUM_HEADS, N_PAGES * PAGE_SIZE, HEAD_DIM, device=device)\n",
        "    physical_v_cache = torch.zeros(1, NUM_HEADS, N_PAGES * PAGE_SIZE, HEAD_DIM, device=device)\n",
        "\n",
        "    # --- 2. Generate Random Data (Ragged Batch) ---\n",
        "    # We will create random KV sequences of different lengths for each batch item.\n",
        "    # We will verify correctness by comparing against standard PyTorch attention.\n",
        "\n",
        "    # Random lengths between 30 and 150 (spanning multiple pages)\n",
        "    # Ensure they are > Q_LEN for this test to be interesting\n",
        "    kv_lengths = torch.randint(Q_LEN + 5, 80, (BATCH_SIZE,), device=device)\n",
        "\n",
        "    print(f\"Test Configuration:\")\n",
        "    print(f\"  Batch Size: {BATCH_SIZE}\")\n",
        "    print(f\"  KV Lengths: {kv_lengths.tolist()}\")\n",
        "    print(f\"  Query Len:  {Q_LEN}\")\n",
        "    print(f\"  Page Size:  {PAGE_SIZE}\")\n",
        "\n",
        "    queries = torch.randn(BATCH_SIZE, NUM_HEADS, Q_LEN, HEAD_DIM, device=device)\n",
        "\n",
        "    # Store ground truth K and V to run reference implementation later\n",
        "    ground_truth_ks = []\n",
        "    ground_truth_vs = []\n",
        "\n",
        "    # --- 3. Populate Paged Attention ---\n",
        "    print(\"\\nPopulating Page Table and Cache...\")\n",
        "\n",
        "    for i in range(BATCH_SIZE):\n",
        "        seq_len = kv_lengths[i].item()\n",
        "\n",
        "        # Generate random K/V for this sequence\n",
        "        k_data = torch.randn(1, NUM_HEADS, seq_len, HEAD_DIM, device=device)\n",
        "        v_data = torch.randn(1, NUM_HEADS, seq_len, HEAD_DIM, device=device)\n",
        "\n",
        "        ground_truth_ks.append(k_data.squeeze(0))\n",
        "        ground_truth_vs.append(v_data.squeeze(0))\n",
        "\n",
        "        # 1. Reserve pages\n",
        "        batch_idx_tensor = torch.tensor([i], device=device)\n",
        "        seq_len_tensor = torch.tensor([seq_len], device=device)\n",
        "        pa.reserve(batch_idx_tensor, seq_len_tensor)\n",
        "\n",
        "        # 2. Assign data\n",
        "        # We assign the whole sequence at once for this test.\n",
        "        # assign expects input_pos of shape [B, S]\n",
        "        input_pos = torch.arange(0, seq_len, device=device).unsqueeze(0) # [1, S]\n",
        "\n",
        "        pa.assign(\n",
        "            batch_idx=batch_idx_tensor,\n",
        "            input_pos=input_pos,\n",
        "            k_val=k_data, # [1, H, S, D]\n",
        "            v_val=v_data, # [1, H, S, D]\n",
        "            k_cache=physical_k_cache,\n",
        "            v_cache=physical_v_cache\n",
        "        )\n",
        "\n",
        "    # --- 4. Run Paged Attention Forward Pass ---\n",
        "    print(\"Running Paged Attention Forward...\")\n",
        "\n",
        "    # Flex Attention scaling default is 1/sqrt(D)\n",
        "    scale = 1.0 / (HEAD_DIM ** 0.5)\n",
        "\n",
        "    # We use a standard Causal Mask implicitly handled inside the PagedAttention.forward\n",
        "    # when block_mask is None.\n",
        "    # Note: In a real \"prefill+decoding\" scenario, Q usually aligns with the END of K.\n",
        "    # Here, we are simulating that Q is attending to the *entire* K history provided.\n",
        "\n",
        "    paged_output = pa.forward(\n",
        "        query=queries,\n",
        "        k_cache=physical_k_cache,\n",
        "        v_cache=physical_v_cache,\n",
        "        kv_lens=kv_lengths,\n",
        "        scale=scale\n",
        "    )\n",
        "\n",
        "    # --- 5. Run Reference Implementation (SDPA) ---\n",
        "    print(\"Running Reference (Standard SDPA)...\")\n",
        "\n",
        "    max_diff = 0.0\n",
        "\n",
        "    for i in range(BATCH_SIZE):\n",
        "        q_i = queries[i]      # [H, Q, D]\n",
        "        k_i = ground_truth_ks[i] # [H, S, D]\n",
        "        v_i = ground_truth_vs[i] # [H, S, D]\n",
        "\n",
        "        # Standard Scaled Dot Product Attention\n",
        "        # We need a causal mask.\n",
        "        # In this specific test setup:\n",
        "        # Query (Length Q) attends to Key (Length S).\n",
        "        # Since we generated S > Q, and usually Q represents the *newest* tokens,\n",
        "        # we need to define the causal relationship.\n",
        "        #\n",
        "        # For simplicity in this test, we will assume standard broadcasting causal mask\n",
        "        # is NOT applied rigidly because S != Q. We simply want to attend to all available keys\n",
        "        # up to the length defined.\n",
        "        # HOWEVER, PagedAttention default mask in previous code was: `q_idx >= k_idx`.\n",
        "        # This implies standard causal masking.\n",
        "\n",
        "        # Let's align reference with the Logic in PagedAttention:\n",
        "        # q_idx (0..Q) attends to k_idx (0..S). Mask is True if q_idx >= k_idx.\n",
        "\n",
        "        # Create explicit mask for SDPA\n",
        "        # Shape: [Q, S]\n",
        "        q_idx = torch.arange(Q_LEN, device=device).unsqueeze(1)\n",
        "        k_idx = torch.arange(kv_lengths[i].item(), device=device).unsqueeze(0)\n",
        "        attn_mask = (q_idx >= k_idx) # Boolean mask\n",
        "\n",
        "        # SDPA expects float mask for add (0, -inf) or boolean (True=Keep, False=Drop)\n",
        "        # But torch.nn.functional.scaled_dot_product_attention handles boolean is_causal\n",
        "        # ONLY if S == Q. Since S != Q, we pass explicit mask.\n",
        "\n",
        "        ref_out = F.scaled_dot_product_attention(\n",
        "            q_i.unsqueeze(0),\n",
        "            k_i.unsqueeze(0),\n",
        "            v_i.unsqueeze(0),\n",
        "            attn_mask=attn_mask.unsqueeze(0).unsqueeze(0),\n",
        "            scale=scale\n",
        "        )\n",
        "\n",
        "        ref_out = ref_out.squeeze(0) # [H, Q, D]\n",
        "\n",
        "        # Compare\n",
        "        pa_out_i = paged_output[i]\n",
        "\n",
        "        diff = (ref_out - pa_out_i).abs().max().item()\n",
        "        max_diff = max(max_diff, diff)\n",
        "\n",
        "        if diff > 1e-3:\n",
        "            print(f\"Batch {i} FAILED. Max Diff: {diff}\")\n",
        "        else:\n",
        "            print(f\"Batch {i} PASSED. Max Diff: {diff:.6f}\")\n",
        "\n",
        "    print(f\"\\nTest Complete. Maximum discrepancy across batch: {max_diff:.6f}\")\n",
        "\n",
        "    if max_diff < 1e-3:\n",
        "        print(\"SUCCESS: Paged Attention matches Reference Implementation.\")\n",
        "    else:\n",
        "        print(\"FAILURE: Differences detected.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Ensure PagedAttention class is available here\n",
        "    test_paged_attention_correctness()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jr3i68evlog4",
        "outputId": "c4250a77-194c-41d5-8448-840664304bf9"
      },
      "id": "Jr3i68evlog4",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Starting Paged Attention Test ===\n",
            "Test Configuration:\n",
            "  Batch Size: 4\n",
            "  KV Lengths: [19, 41, 62, 19]\n",
            "  Query Len:  12\n",
            "  Page Size:  16\n",
            "\n",
            "Populating Page Table and Cache...\n",
            "Running Paged Attention Forward...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/attention/flex_attention.py:1687: UserWarning: flex_attention called without torch.compile() - this will use an unfused implementation that materializes the full scores matrix instead of generating a fused kernel.\n",
            "\n",
            "SOLUTION: Use torch.compile(flex_attention)(...)\n",
            "\n",
            "If you want to debug your score_mod/mask_mod, you can set:\n",
            "torch.nn.attention.flex_attention._FLEX_ATTENTION_DISABLE_COMPILE_DEBUG = True\n",
            "\n",
            "This will allow you to use print statements or breakpoints. Note: This doesn't work with the backwards pass and may produce incorrect results.\n",
            "  _warn_once(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Reference (Standard SDPA)...\n",
            "Batch 0 PASSED. Max Diff: 0.000001\n",
            "Batch 1 PASSED. Max Diff: 0.000001\n",
            "Batch 2 PASSED. Max Diff: 0.000001\n",
            "Batch 3 PASSED. Max Diff: 0.000001\n",
            "\n",
            "Test Complete. Maximum discrepancy across batch: 0.000001\n",
            "SUCCESS: Paged Attention matches Reference Implementation.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}