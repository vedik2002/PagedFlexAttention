{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8583ea76-f4c6-4b4b-9dcd-a07749a29909",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Granite 4 + DeepEval MMLU + TruthfulQA\n",
    "\n",
    "Metrics:\n",
    "  accuracy, latency (ms/query), throughput, GPU memory,\n",
    "  batching efficiency, context length scalability\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import statistics\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any, Optional, Type\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from datasets import load_dataset, DownloadConfig\n",
    "from deepeval.models import DeepEvalBaseLLM\n",
    "from deepeval.benchmarks import MMLU, TruthfulQA\n",
    "from deepeval.benchmarks.mmlu.task import MMLUTask\n",
    "from deepeval.benchmarks.tasks import TruthfulQATask\n",
    "from deepeval.benchmarks.modes import TruthfulQAMode\n",
    "\n",
    "\n",
    "# Hugging Face home Directory\n",
    "#___________________\n",
    "HF_CACHE_DIR = os.environ.get(\"HF_HOME\", os.path.expanduser(\"~/.cache/huggingface\"))\n",
    "os.environ.setdefault(\"HF_HOME\", HF_CACHE_DIR)\n",
    "os.environ.setdefault(\"HF_DATASETS_CACHE\", os.path.join(HF_CACHE_DIR, \"datasets\"))\n",
    "\n",
    "def load_dataset_with_retry(\n",
    "    path: str,\n",
    "    name: Optional[str] = None,\n",
    "    split: Optional[str] = None,\n",
    "    retries: int = 6,\n",
    "    base_delay: float = 2.0,\n",
    "    **kwargs,\n",
    "):\n",
    "    \"\"\"\n",
    "    Load hugging face dataset. Added try catch to prevent download/access errors \n",
    "    \"\"\"\n",
    "    download_config = kwargs.pop(\"download_config\", None) or DownloadConfig(\n",
    "        resume_download=True,\n",
    "        max_retries=1,\n",
    "    )\n",
    "\n",
    "    last_err = None\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            return load_dataset(\n",
    "                path,\n",
    "                name=name,\n",
    "                split=split,\n",
    "                download_config=download_config,\n",
    "                **kwargs,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            msg = str(e)\n",
    "            transient = any(s in msg for s in [\n",
    "                \" 502 \", \" 503 \", \" 504 \",\n",
    "                \"Bad Gateway\", \"Service Unavailable\", \"Gateway\",\n",
    "                \"Read timed out\", \"Connection reset\", \"Temporary failure\",\n",
    "                \"MaxRetryError\", \"HTTPSConnectionPool\",\n",
    "            ])\n",
    "            if not transient and attempt >= 1:\n",
    "                raise\n",
    "            sleep_s = base_delay * (2 ** attempt) + random.random()\n",
    "            print(f\"[dataset retry] attempt {attempt+1}/{retries} failed: {e}\\n -> sleeping {sleep_s:.1f}s\")\n",
    "            time.sleep(sleep_s)\n",
    "    raise last_err\n",
    "\n",
    "\n",
    "#helper functions\n",
    "#_____________________\n",
    "def _cuda_sync_if_needed(device: str):\n",
    "    \"\"\"\n",
    "    use syncronize to ensure accuracy\n",
    "    \"\"\"\n",
    "    if device.startswith(\"cuda\") and torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "def _percentile(xs: List[float], p: float) -> float:\n",
    "    \"\"\"\n",
    "    calculate the percentile with linear interpolation\n",
    "    \"\"\"\n",
    "    if not xs:\n",
    "        return float(\"nan\")\n",
    "    xs_sorted = sorted(xs)\n",
    "    k = (len(xs_sorted) - 1) * p\n",
    "    f = math.floor(k)\n",
    "    c = math.ceil(k)\n",
    "    if f == c:\n",
    "        return xs_sorted[int(k)]\n",
    "    return xs_sorted[f] * (c - k) + xs_sorted[c] * (k - f)\n",
    "\n",
    "def _format_bytes(n: Optional[float]) -> str:\n",
    "    \"\"\"\n",
    "    format bytes to to string representation \n",
    "    \"\"\"\n",
    "    if n is None or (isinstance(n, float) and math.isnan(n)):\n",
    "        return \"n/a\"\n",
    "    units = [\"B\", \"KB\", \"MB\", \"GB\", \"TB\"]\n",
    "    u = 0\n",
    "    n = float(n)\n",
    "    while n >= 1024 and u < len(units) - 1:\n",
    "        n /= 1024\n",
    "        u += 1\n",
    "    return f\"{n:.2f} {units[u]}\"\n",
    "\n",
    "\n",
    "#metrics\n",
    "#______________\n",
    "@dataclass\n",
    "class CallMetrics:\n",
    "    latency_ms: float\n",
    "    input_tokens: int\n",
    "    output_tokens: int\n",
    "    peak_alloc_bytes: Optional[int] = None\n",
    "    peak_reserved_bytes: Optional[int] = None\n",
    "\n",
    "@dataclass\n",
    "class RunSummary:\n",
    "    n_calls: int\n",
    "    avg_latency_ms: float\n",
    "    p50_latency_ms: float\n",
    "    p95_latency_ms: float\n",
    "    qps: float\n",
    "    tok_per_s_in: float\n",
    "    tok_per_s_out: float\n",
    "    peak_alloc_bytes_max: Optional[int]\n",
    "    peak_reserved_bytes_max: Optional[int]\n",
    "\n",
    "def summarize_calls(calls: List[CallMetrics]) -> RunSummary:\n",
    "    n = len(calls)\n",
    "    lats = [c.latency_ms for c in calls]\n",
    "    in_toks = [c.input_tokens for c in calls]\n",
    "    out_toks = [c.output_tokens for c in calls]\n",
    "    total_s = sum(lats) / 1000.0 if lats else 0.0\n",
    "\n",
    "    peak_allocs = [c.peak_alloc_bytes for c in calls if c.peak_alloc_bytes is not None]\n",
    "    peak_resvs = [c.peak_reserved_bytes for c in calls if c.peak_reserved_bytes is not None]\n",
    "\n",
    "    return RunSummary(\n",
    "        n_calls=n,\n",
    "        avg_latency_ms=(statistics.mean(lats) if lats else float(\"nan\")),\n",
    "        p50_latency_ms=_percentile(lats, 0.50),\n",
    "        p95_latency_ms=_percentile(lats, 0.95),\n",
    "        qps=(n / total_s if total_s > 0 else float(\"nan\")),\n",
    "        tok_per_s_in=(sum(in_toks) / total_s if total_s > 0 else float(\"nan\")),\n",
    "        tok_per_s_out=(sum(out_toks) / total_s if total_s > 0 else float(\"nan\")),\n",
    "        peak_alloc_bytes_max=(max(peak_allocs) if peak_allocs else None),\n",
    "        peak_reserved_bytes_max=(max(peak_resvs) if peak_resvs else None),\n",
    "    )\n",
    "\n",
    "\n",
    "# =========================\n",
    "# DeepEval \"schema\" return shim\n",
    "# =========================\n",
    "class AnswerObj:\n",
    "    \"\"\"Minimal object compatible with DeepEval expecting `.answer`.\"\"\"\n",
    "    def __init__(self, answer: str):\n",
    "        self.answer = answer\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Granite wrapper (structured schema + single-device model)\n",
    "# =========================\n",
    "class Granite4HF(DeepEvalBaseLLM):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_path: str = \"ibm-granite/granite-4.0-h-1b\",\n",
    "        device: str = \"cuda\",\n",
    "        max_new_tokens: int = 16,\n",
    "        dtype: Optional[torch.dtype] = None,\n",
    "    ):\n",
    "        self.model_path = model_path\n",
    "        self.device = device\n",
    "        self.max_new_tokens = max_new_tokens\n",
    "\n",
    "        self._is_cuda = device.startswith(\"cuda\") and torch.cuda.is_available()\n",
    "\n",
    "        if dtype is None:\n",
    "            dtype = torch.float16 if self._is_cuda else torch.float32\n",
    "        self.dtype = dtype\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "        # Prefer dtype=... (newer); fall back to torch_dtype=... (older)\n",
    "        try:\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(model_path, dtype=self.dtype)\n",
    "        except TypeError:\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=self.dtype)\n",
    "\n",
    "        self.model.to(device)\n",
    "        self.model.eval()\n",
    "\n",
    "        self.calls: List[CallMetrics] = []\n",
    "        if self._is_cuda:\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "    def reset_metrics(self):\n",
    "        self.calls.clear()\n",
    "        if self._is_cuda:\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "    def get_model_name(self) -> str:\n",
    "        return self.model_path\n",
    "\n",
    "    def load_model(self):\n",
    "        return self.model\n",
    "\n",
    "    def _extract_choice_token(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        For MC tasks, DeepEval wants a compact option token.\n",
    "        We try:\n",
    "          - A-E\n",
    "          - A-D (common)\n",
    "          - otherwise return stripped text\n",
    "        \"\"\"\n",
    "        t = text.strip()\n",
    "        m = re.search(r\"\\b([A-E])\\b\", t)\n",
    "        return m.group(1) if m else t\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def generate(self, prompt: Optional[str] = None, **kwargs):\n",
    "        \"\"\"\n",
    "        DeepEval may call:\n",
    "          generate(prompt=..., schema=MultipleChoiceSchema)\n",
    "        If `schema` is provided, return an object with `.answer`.\n",
    "        Otherwise return a plain string.\n",
    "        \"\"\"\n",
    "        if prompt is None:\n",
    "            prompt = kwargs.get(\"prompt\", \"\")\n",
    "\n",
    "        schema: Optional[Type] = kwargs.get(\"schema\", None)\n",
    "\n",
    "        if self._is_cuda:\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\")\n",
    "        input_len = int(inputs[\"input_ids\"].shape[-1])\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "\n",
    "        _cuda_sync_if_needed(self.device)\n",
    "        t0 = time.perf_counter()\n",
    "\n",
    "        out = self.model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=self.max_new_tokens,\n",
    "            do_sample=False,\n",
    "            temperature=0.0,\n",
    "            top_p=1.0,\n",
    "            use_cache=True,\n",
    "            pad_token_id=self.tokenizer.eos_token_id,\n",
    "            eos_token_id=self.tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "        _cuda_sync_if_needed(self.device)\n",
    "        t1 = time.perf_counter()\n",
    "\n",
    "        gen_text = self.tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "        output_len = int(out.shape[-1])\n",
    "        new_tokens = max(0, output_len - input_len)\n",
    "\n",
    "        completion = gen_text[len(prompt):] if len(gen_text) >= len(prompt) else gen_text\n",
    "        answer = self._extract_choice_token(completion)\n",
    "\n",
    "        latency_ms = (t1 - t0) * 1000.0\n",
    "        peak_alloc = int(torch.cuda.max_memory_allocated()) if self._is_cuda else None\n",
    "        peak_resv = int(torch.cuda.max_memory_reserved()) if self._is_cuda else None\n",
    "\n",
    "        self.calls.append(\n",
    "            CallMetrics(\n",
    "                latency_ms=latency_ms,\n",
    "                input_tokens=input_len,\n",
    "                output_tokens=new_tokens,\n",
    "                peak_alloc_bytes=peak_alloc,\n",
    "                peak_reserved_bytes=peak_resv,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # If DeepEval requested a schema, return an object with `.answer`\n",
    "        if schema is not None:\n",
    "            # Some DeepEval paths might accept a schema class instance; we keep it minimal.\n",
    "            return AnswerObj(answer)\n",
    "\n",
    "        return answer\n",
    "\n",
    "    async def a_generate(self, prompt: str, **kwargs):\n",
    "        return self.generate(prompt=prompt, **kwargs)\n",
    "\n",
    "\n",
    "#MMLU\n",
    "#__________________\n",
    "class MMLU_Retry(MMLU):\n",
    "    def load_benchmark_dataset(self, task: MMLUTask):\n",
    "        self.dataset = load_dataset_with_retry(\"cais/mmlu\", name=task.value)\n",
    "        return super().load_benchmark_dataset(task)\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def bench_batching_efficiency(\n",
    "    model: AutoModelForCausalLM,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    device: str,\n",
    "    base_prompt: str,\n",
    "    batch_sizes: List[int],\n",
    "    max_new_tokens: int = 16,\n",
    "    warmup: int = 1,\n",
    "    iters: int = 5,\n",
    ") -> List[Dict[str, Any]]:\n",
    "    is_cuda = device.startswith(\"cuda\") and torch.cuda.is_available()\n",
    "    results = []\n",
    "\n",
    "    for bs in batch_sizes:\n",
    "        prompts = [base_prompt] * bs\n",
    "        enc = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        enc = {k: v.to(device) for k, v in enc.items()}\n",
    "\n",
    "        for _ in range(warmup):\n",
    "            _cuda_sync_if_needed(device)\n",
    "            _ = model.generate(\n",
    "                **enc,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=False,\n",
    "                temperature=0.0,\n",
    "                top_p=1.0,\n",
    "                use_cache=True,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "            _cuda_sync_if_needed(device)\n",
    "\n",
    "        lat_ms_runs = []\n",
    "        out_tokens_runs = []\n",
    "        in_tokens_total = int(enc[\"input_ids\"].numel())\n",
    "\n",
    "        for _ in range(iters):\n",
    "            if is_cuda:\n",
    "                torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "            _cuda_sync_if_needed(device)\n",
    "            t0 = time.perf_counter()\n",
    "\n",
    "            out = model.generate(\n",
    "                **enc,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=False,\n",
    "                temperature=0.0,\n",
    "                top_p=1.0,\n",
    "                use_cache=True,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "\n",
    "            _cuda_sync_if_needed(device)\n",
    "            t1 = time.perf_counter()\n",
    "\n",
    "            lat_ms = (t1 - t0) * 1000.0\n",
    "            lat_ms_runs.append(lat_ms)\n",
    "\n",
    "            out_tokens_total = int(out.numel()) - int(enc[\"input_ids\"].numel())\n",
    "            out_tokens_runs.append(max(0, out_tokens_total))\n",
    "\n",
    "        avg_lat_ms = statistics.mean(lat_ms_runs)\n",
    "        total_s = avg_lat_ms / 1000.0\n",
    "\n",
    "        results.append(\n",
    "            {\n",
    "                \"batch_size\": bs,\n",
    "                \"avg_batch_latency_ms\": avg_lat_ms,\n",
    "                \"latency_ms_per_query\": avg_lat_ms / bs,\n",
    "                \"qps\": (bs / total_s) if total_s > 0 else float(\"nan\"),\n",
    "                \"in_tokens_per_s\": (in_tokens_total / total_s) if total_s > 0 else float(\"nan\"),\n",
    "                \"out_tokens_per_s\": (statistics.mean(out_tokens_runs) / total_s) if total_s > 0 else float(\"nan\"),\n",
    "                \"peak_alloc\": int(torch.cuda.max_memory_allocated()) if is_cuda else None,\n",
    "                \"peak_reserved\": int(torch.cuda.max_memory_reserved()) if is_cuda else None,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def bench_context_length_scalability(\n",
    "    model: AutoModelForCausalLM,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    device: str,\n",
    "    token_lengths: List[int],\n",
    "    max_new_tokens: int = 8,\n",
    "    warmup: int = 1,\n",
    "    iters: int = 5,\n",
    ") -> List[Dict[str, Any]]:\n",
    "    is_cuda = device.startswith(\"cuda\") and torch.cuda.is_available()\n",
    "    results = []\n",
    "    chunk = \" The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "    for target_len in token_lengths:\n",
    "        prompt = \"\"\n",
    "        while len(tokenizer(prompt).input_ids) < target_len:\n",
    "            prompt += chunk\n",
    "\n",
    "        enc = tokenizer(prompt, return_tensors=\"pt\")\n",
    "        enc = {k: v.to(device) for k, v in enc.items()}\n",
    "        in_len = int(enc[\"input_ids\"].shape[-1])\n",
    "\n",
    "        for _ in range(warmup):\n",
    "            _cuda_sync_if_needed(device)\n",
    "            _ = model.generate(\n",
    "                **enc,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=False,\n",
    "                temperature=0.0,\n",
    "                top_p=1.0,\n",
    "                use_cache=True,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "            _cuda_sync_if_needed(device)\n",
    "\n",
    "        lats = []\n",
    "        for _ in range(iters):\n",
    "            if is_cuda:\n",
    "                torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "            _cuda_sync_if_needed(device)\n",
    "            t0 = time.perf_counter()\n",
    "\n",
    "            _ = model.generate(\n",
    "                **enc,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=False,\n",
    "                temperature=0.0,\n",
    "                top_p=1.0,\n",
    "                use_cache=True,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "\n",
    "            _cuda_sync_if_needed(device)\n",
    "            t1 = time.perf_counter()\n",
    "            lats.append((t1 - t0) * 1000.0)\n",
    "\n",
    "        results.append(\n",
    "            {\n",
    "                \"input_tokens\": in_len,\n",
    "                \"avg_latency_ms\": statistics.mean(lats),\n",
    "                \"p95_latency_ms\": _percentile(lats, 0.95),\n",
    "                \"ms_per_input_token\": statistics.mean(lats) / max(1, in_len),\n",
    "                \"peak_alloc\": int(torch.cuda.max_memory_allocated()) if is_cuda else None,\n",
    "                \"peak_reserved\": int(torch.cuda.max_memory_reserved()) if is_cuda else None,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def print_run_report(title: str, overall_score: float, calls: List[CallMetrics]):\n",
    "    s = summarize_calls(calls)\n",
    "    print(f\"\\n=== {title} ===\")\n",
    "    print(f\"Accuracy (overall_score): {overall_score:.4f}\")\n",
    "    print(f\"Queries: {s.n_calls}\")\n",
    "    print(f\"Latency (ms/query): avg={s.avg_latency_ms:.2f}, p50={s.p50_latency_ms:.2f}, p95={s.p95_latency_ms:.2f}\")\n",
    "    print(f\"Throughput: {s.qps:.2f} queries/s\")\n",
    "    print(f\"Token throughput: in={s.tok_per_s_in:.2f} tok/s, out={s.tok_per_s_out:.2f} tok/s\")\n",
    "    print(f\"Peak GPU memory (allocated): {_format_bytes(s.peak_alloc_bytes_max)}\")\n",
    "    print(f\"Peak GPU memory (reserved):  {_format_bytes(s.peak_reserved_bytes_max)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74530e4a-5cb9-4c91-868a-9686cd0f6307",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing high_school_computer_science: 100%|██████████| 100/100 [1:52:41<00:00, 67.61s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMLU Task Accuracy (task=high_school_computer_science): 0.66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing astronomy: 100%|██████████| 152/152 [1:44:55<00:00, 41.42s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMLU Task Accuracy (task=astronomy): 0.6842105263157895\n",
      "Overall MMLU Accuracy: 0.6746031746031746\n",
      "\n",
      "=== MMLU Results + Metrics ===\n",
      "Accuracy (overall_score): 0.6746\n",
      "Queries: 252\n",
      "Latency (ms/query): avg=51801.93, p50=61173.47, p95=68055.21\n",
      "Throughput: 0.02 queries/s\n",
      "Token throughput: in=7.91 tok/s, out=0.11 tok/s\n",
      "Peak GPU memory (allocated): 12.46 GB\n",
      "Peak GPU memory (reserved):  15.04 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "C:\\Users\\Jason\\anaconda3\\envs\\Python3_12_12\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Jason\\.cache\\huggingface\\hub\\datasets--truthful_qa. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Generating validation split: 100%|██████████| 817/817 [00:00<00:00, 41501.11 examples/s]\n",
      "Filter: 100%|██████████| 817/817 [00:00<00:00, 26011.83 examples/s]\n",
      "Processing Science: 100%|██████████| 9/9 [03:29<00:00, 23.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TruthfulQA Task Accuracy (task=Science): 0.2222222222222222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 817/817 [00:00<00:00, 38897.42 examples/s]\n",
      "Processing Finance: 100%|██████████| 9/9 [03:30<00:00, 23.35s/it]\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TruthfulQA Task Accuracy (task=Finance): 0.4444444444444444\n",
      "Overall TruthfulQA Accuracy: 0.3333333333333333\n",
      "\n",
      "=== TruthfulQA Results + Metrics ===\n",
      "Accuracy (overall_score): 0.3333\n",
      "Queries: 18\n",
      "Latency (ms/query): avg=23332.34, p50=23130.24, p95=24400.79\n",
      "Throughput: 0.04 queries/s\n",
      "Token throughput: in=13.78 tok/s, out=0.26 tok/s\n",
      "Peak GPU memory (allocated): 10.14 GB\n",
      "Peak GPU memory (reserved):  15.04 GB\n",
      "\n",
      "=== Batching Efficiency Microbenchmark ===\n",
      "BS= 1 | ms/query=13172.92 | qps=0.08 | in_tok/s=3 | out_tok/s=0 | peak_alloc=7.80 GB\n",
      "BS= 2 | ms/query=30021.99 | qps=0.03 | in_tok/s=1 | out_tok/s=0 | peak_alloc=10.15 GB\n",
      "BS= 4 | ms/query=31226.30 | qps=0.03 | in_tok/s=1 | out_tok/s=0 | peak_alloc=14.84 GB\n",
      "BS= 8 | ms/query=32150.16 | qps=0.03 | in_tok/s=1 | out_tok/s=0 | peak_alloc=24.23 GB\n",
      "BS=16 | ms/query=24966.52 | qps=0.04 | in_tok/s=2 | out_tok/s=0 | peak_alloc=43.00 GB\n",
      "\n",
      "=== Context Length Scalability Microbenchmark ===\n",
      "Using max_position_embeddings=131072\n",
      "in_tokens= 130 | avg_ms=13095.30 | p95_ms=13122.43 | ms/token=100.7330 | peak_alloc=7.81 GB\n",
      "in_tokens= 260 | avg_ms=25553.38 | p95_ms=25624.03 | ms/token=98.2822 | peak_alloc=10.13 GB\n",
      "in_tokens= 520 | avg_ms=38798.41 | p95_ms=39336.87 | ms/token=74.6123 | peak_alloc=12.46 GB\n",
      "in_tokens=1030 | avg_ms=63865.56 | p95_ms=63899.02 | ms/token=62.0054 | peak_alloc=17.11 GB\n",
      "in_tokens=2050 | avg_ms=106821.78 | p95_ms=106942.23 | ms/token=52.1082 | peak_alloc=26.42 GB\n",
      "in_tokens=4100 | avg_ms=208664.10 | p95_ms=208882.71 | ms/token=50.8937 | peak_alloc=45.06 GB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_path = \"ibm-granite/granite-4.0-h-1b\"\n",
    "\n",
    "granite = Granite4HF(model_path=model_path, device=device, max_new_tokens=16)\n",
    "\n",
    "# -------- MMLU --------\n",
    "granite.reset_metrics()\n",
    "mmlu = MMLU_Retry(\n",
    "    tasks=[MMLUTask.HIGH_SCHOOL_COMPUTER_SCIENCE, MMLUTask.ASTRONOMY],\n",
    "    n_shots=3,\n",
    ")\n",
    "mmlu.evaluate(model=granite)\n",
    "print_run_report(\"MMLU Results + Metrics\", mmlu.overall_score, granite.calls)\n",
    "\n",
    "# ----- TruthfulQA -----\n",
    "granite.reset_metrics()\n",
    "truthfulqa = TruthfulQA(\n",
    "    tasks=[TruthfulQATask.SCIENCE, TruthfulQATask.FINANCE],\n",
    "    mode=TruthfulQAMode.MC1,\n",
    ")\n",
    "truthfulqa.evaluate(model=granite)\n",
    "print_run_report(\"TruthfulQA Results + Metrics\", truthfulqa.overall_score, granite.calls)\n",
    "\n",
    "# -------- Optional microbenchmarks --------\n",
    "hf_model = granite.model\n",
    "hf_tok = granite.tokenizer\n",
    "\n",
    "print(\"\\n=== Batching Efficiency Microbenchmark ===\")\n",
    "base_prompt = (\n",
    "    \"Answer with ONLY one letter: A, B, C, or D.\\n\"\n",
    "    \"Question: Which planet is known as the Red Planet?\\n\"\n",
    "    \"A) Venus\\nB) Mars\\nC) Jupiter\\nD) Mercury\\n\"\n",
    "    \"Answer:\"\n",
    ")\n",
    "batch_results = bench_batching_efficiency(\n",
    "    model=hf_model,\n",
    "    tokenizer=hf_tok,\n",
    "    device=device,\n",
    "    base_prompt=base_prompt,\n",
    "    batch_sizes=[1, 2, 4, 8, 16],\n",
    "    max_new_tokens=8,\n",
    "    warmup=1,\n",
    "    iters=5,\n",
    ")\n",
    "for r in batch_results:\n",
    "    print(\n",
    "        f\"BS={r['batch_size']:>2} | ms/query={r['latency_ms_per_query']:.2f} | \"\n",
    "        f\"qps={r['qps']:.2f} | in_tok/s={r['in_tokens_per_s']:.0f} | out_tok/s={r['out_tokens_per_s']:.0f} | \"\n",
    "        f\"peak_alloc={_format_bytes(r['peak_alloc'])}\"\n",
    "    )\n",
    "\n",
    "print(\"\\n=== Context Length Scalability Microbenchmark ===\")\n",
    "max_ctx = getattr(getattr(hf_model, \"config\", None), \"max_position_embeddings\", None) or 4096\n",
    "token_lengths = [t for t in [128, 256, 512, 1024, 2048, 4096] if t <= max_ctx]\n",
    "\n",
    "ctx_results = bench_context_length_scalability(\n",
    "    model=hf_model,\n",
    "    tokenizer=hf_tok,\n",
    "    device=device,\n",
    "    token_lengths=token_lengths,\n",
    "    max_new_tokens=8,\n",
    "    warmup=1,\n",
    "    iters=5, \n",
    ")\n",
    "print(f\"Using max_position_embeddings={max_ctx}\")\n",
    "for r in ctx_results:\n",
    "    print(\n",
    "        f\"in_tokens={r['input_tokens']:>4} | avg_ms={r['avg_latency_ms']:.2f} | p95_ms={r['p95_latency_ms']:.2f} | \"\n",
    "        f\"ms/token={r['ms_per_input_token']:.4f} | peak_alloc={_format_bytes(r['peak_alloc'])}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5f359a-5713-431f-96e0-01a72d72db87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
